{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "074df3ae-4bfc-4655-be9f-8041fc211f96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# code from mindnlp and huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a161bfb-a15e-4a07-9bb0-688b76f87de3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore\n",
    "from mindspore import nn, ops, Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151fbcbf-b579-4e28-95fa-2e0ab4e47f7a",
   "metadata": {},
   "source": [
    "# GPT-2 Masked Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab3a91-292b-420d-9b61-c85280dd8dee",
   "metadata": {},
   "source": [
    "## GPT-2 Self-attention: 1- Creating queries, keys, and values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22664691-6db2-4d62-a76a-a4a8a6050199",
   "metadata": {},
   "source": [
    "![gpt2-self-attention-3.png](https://jalammar.github.io/images/gpt2/gpt2-self-attention-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d2591f-26c2-4961-bca3-be30c4352aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 10\n",
    "embed_dim = 768\n",
    "\n",
    "x = Tensor(np.random.randn(batch_size, seq_len, embed_dim), mindspore.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d315a4e-5663-404e-b93d-efb1cf354414",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lvyufeng/miniconda3/envs/mindspore/lib/python3.7/site-packages/mindnlp/utils/download.py:26: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1, 10, 768), (1, 10, 768), (1, 10, 768))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindnlp._legacy.functional import split\n",
    "from mindnlp.models.utils.utils import Conv1D\n",
    "\n",
    "c_attn = Conv1D(3 * embed_dim, embed_dim)\n",
    "query, key, value = split(c_attn(x), embed_dim, axis=2)\n",
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c7757e-16e4-4ff9-8a63-3e19767588db",
   "metadata": {},
   "source": [
    "![gpt2-self-attention-split-attention-heads-1.png](https://jalammar.github.io/images/gpt2/gpt2-self-attention-split-attention-heads-1.png)\n",
    "\n",
    "![gpt2-self-attention-split-attention-heads-2.png](https://jalammar.github.io/images/gpt2/gpt2-self-attention-split-attention-heads-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb7ccac-7cfe-401a-ab32-763de70b4669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_heads(tensor, num_heads, attn_head_size):\n",
    "    \"\"\"\n",
    "    Splits hidden_size dim into attn_head_size and num_heads\n",
    "    \"\"\"\n",
    "    new_shape = tensor.shape[:-1] + (num_heads, attn_head_size)\n",
    "    tensor = tensor.view(new_shape)\n",
    "    return ops.transpose(tensor, (0, 2, 1, 3))  # (batch, head, seq_length, head_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72abe0fe-5225-425b-9bda-0723f3fb27cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 12, 10, 64), (1, 12, 10, 64), (1, 12, 10, 64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads = 12\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "query = split_heads(query, num_heads, head_dim)\n",
    "key = split_heads(key, num_heads, head_dim)\n",
    "value = split_heads(value, num_heads, head_dim)\n",
    "\n",
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0f65b2-b291-4ad1-b3ea-8e77e6a254d3",
   "metadata": {},
   "source": [
    "## GPT-2 Self-attention: 2- Scoring\n",
    "\n",
    "![gpt2-self-attention-scoring.png](https://jalammar.github.io/images/gpt2/gpt2-self-attention-scoring.png)\n",
    "\n",
    "![](https://jalammar.github.io/images/gpt2/gpt2-self-attention-scoring-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f952236-de74-4419-9469-7e78d3b7c3e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12, 10, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = ops.matmul(query, key.swapaxes(-1, -2))\n",
    "\n",
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d6de9-cdb7-40cd-aed1-e4fe059054b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](https://jalammar.github.io/images/gpt2/transformer-decoder-attention-mask-dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ff22248-deff-4962-afae-55772f63f142",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[1, 1, 10, 10], dtype=Bool, value=\n",
       "[[[[ True, False, False ... False, False, False],\n",
       "   [ True,  True, False ... False, False, False],\n",
       "   [ True,  True,  True ... False, False, False],\n",
       "   ...\n",
       "   [ True,  True,  True ...  True, False, False],\n",
       "   [ True,  True,  True ...  True,  True, False],\n",
       "   [ True,  True,  True ...  True,  True,  True]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_positions = seq_len\n",
    "\n",
    "bias = Tensor(np.tril(np.ones((max_positions, max_positions))).reshape(\n",
    "              (1, 1, max_positions, max_positions)), mindspore.bool_)\n",
    "bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a783a2bc-01dd-4496-a018-ac01e643cd89",
   "metadata": {},
   "source": [
    "![](https://jalammar.github.io/images/gpt2/queries-keys-attention-mask.png)\n",
    "\n",
    "![](https://jalammar.github.io/images/gpt2/transformer-attention-mask.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d957ce17-6df6-4f5e-a262-24ff3a8ce0d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindnlp._legacy.functional import where, softmax\n",
    "\n",
    "attn_weights = attn_weights / ops.sqrt(ops.scalar_to_tensor(value.shape[-1]))\n",
    "query_length, key_length = query.shape[-2], key.shape[-2]\n",
    "causal_mask = bias[:, :, key_length - query_length: key_length, :key_length].bool()\n",
    "mask_value = Tensor(np.finfo(np.float32).min, dtype=attn_weights.dtype)\n",
    "attn_weights = where(causal_mask, attn_weights, mask_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dee63bfd-f394-4558-9e9f-e102a2fd283c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.4028235e+38"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.finfo(np.float32).min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2faad14-9a3d-4495-8bcc-d7ac2695e83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[10, 10], dtype=Float32, value=\n",
       "[[ 1.45791307e-01, -3.40282347e+38, -3.40282347e+38 ... -3.40282347e+38, -3.40282347e+38, -3.40282347e+38],\n",
       " [ 3.03372920e-01,  1.87853992e-01, -3.40282347e+38 ... -3.40282347e+38, -3.40282347e+38, -3.40282347e+38],\n",
       " [ 2.68798053e-01, -4.71530288e-01, -7.39960253e-01 ... -3.40282347e+38, -3.40282347e+38, -3.40282347e+38],\n",
       " ...\n",
       " [-6.35852873e-01,  2.11123060e-02,  8.07071626e-02 ...  1.21709414e-01, -3.40282347e+38, -3.40282347e+38],\n",
       " [-3.82918388e-01, -1.46051317e-01, -2.59720534e-01 ...  1.63411722e-01, -8.62300470e-02, -3.40282347e+38],\n",
       " [ 1.63633123e-01, -1.29443496e-01, -3.18507515e-02 ... -1.13381080e-01,  5.03538430e-01,  1.39019817e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f61883-a535-4135-851c-c41e9c227e18",
   "metadata": {},
   "source": [
    "![](https://jalammar.github.io/images/gpt2/transformer-attention-masked-scores-softmax.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df9cdaae-ac5a-4bc0-9e59-403d176c0d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12, 10, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = softmax(attn_weights, axis=-1)\n",
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5771f68c-8b35-4b1a-83d1-287b2ce7a47e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[10, 10], dtype=Float32, value=\n",
       "[[ 1.00000000e+00,  0.00000000e+00,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       " [ 5.28847635e-01,  4.71152276e-01,  0.00000000e+00 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       " [ 5.42997599e-01,  2.58986652e-01,  1.98015764e-01 ...  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       " ...\n",
       " [ 6.78379014e-02,  1.30854547e-01,  1.38889834e-01 ...  1.44702986e-01,  0.00000000e+00,  0.00000000e+00],\n",
       " [ 8.35228637e-02,  1.05846249e-01,  9.44733843e-02 ...  1.44235939e-01,  1.12371311e-01,  0.00000000e+00],\n",
       " [ 1.19870029e-01,  8.94188508e-02,  9.85854939e-02 ...  9.08667147e-02,  1.68395162e-01,  1.16955645e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a376e6b-0cd8-434a-aa5b-c647251200fa",
   "metadata": {},
   "source": [
    "![](https://jalammar.github.io/images/gpt2/gpt2-self-attention-multihead-sum-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ba1e0ff-5627-4b70-8911-4ffa7383e29d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12, 10, 64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output = ops.matmul(attn_weights, value)\n",
    "\n",
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952ef91-b1e1-4d5b-9b42-a29f56f8f430",
   "metadata": {},
   "source": [
    "## GPT-2 Self-attention: 3.5- Merge attention heads\n",
    "\n",
    "![](https://jalammar.github.io/images/gpt2/gpt2-self-attention-merge-heads-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80e44dd1-4013-4d01-b267-92463b296e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_heads(tensor, num_heads, attn_head_size):\n",
    "    \"\"\"\n",
    "    Merges attn_head_size dim and num_attn_heads dim into hidden_size\n",
    "    \"\"\"\n",
    "    tensor = ops.transpose(tensor, (0, 2, 1, 3))\n",
    "    new_shape = tensor.shape[:-2] + (num_heads * attn_head_size,)\n",
    "    return tensor.view(new_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b35f8ee-70b4-4cb4-ad9b-d0b685482b59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output = merge_heads(attn_output, num_heads, head_dim)\n",
    "\n",
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14b271-4432-44a0-b1f9-d2632ed2cd5b",
   "metadata": {},
   "source": [
    "## GPT-2 Self-attention: 4- Projecting\n",
    "\n",
    "![](https://jalammar.github.io/images/gpt2/gpt2-self-attention-project-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff788df6-a6a7-4b43-9a76-95eaef4918c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_proj = Conv1D(embed_dim, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c7d4c1f-4ddc-4605-acba-f6e17cbfe2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 768)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output = c_proj(attn_output)\n",
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300497c-e27a-4fad-b02e-fe2b6a38aec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
