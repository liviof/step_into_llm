# 昇思MindSpore技术公开课

- ***探究前沿***：解读技术热点，解构热点模型
- ***应用实践***：理论实践相结合，手把手指导开发
- ***专家解读***：多领域专家，多元解读
- ***开源共享***：课程免费，课件代码开源
- ***大赛赋能***：ICT大赛赋能课程（大模型专题第一、二期）
- ***系列课程***：大模型专题课程开展中，其他专题课程敬请期待

## 报名方式

报名链接：https://xihe.mindspore.cn/course/foundation-model-v2/introduction 

（注：参与免费课程必须报名哦！同步添加QQ群，后续课程事宜将在群内通知！）

## 大模型专题第二期（进行中）

紧跟前沿技术，解构热点大模型（如ChatGLM2、LLAMA2等）；手把手教你大模型从开发到应用全流程

课程资料归档：[link](./Season2.step_into_llm/)

### 教研团队

- 孙显 

    中国科学院空天信息创新研究院研究员 实验室副主任

- 龚柏涛（面壁智能）

    OpenBMB开源社区技术负责人 清华大学硕士 CPM-Bee开源大模型项目主要维护者

- 杨琨（达闼科技）

    解决方案工程师

- 李宁（达闼科技）

    解决方案经理

- 周汝霖 

    昇思MindSpore布道师 深圳大学华为智能基座社长 2022年华为昇思十大优秀开发者

- CQU弟中弟

    昇思MindSpore易用性专家 昇思MindSpore布道师

- Eric 

    昇思MindSpore模型压缩技术专家 昇思MindSpore布道师

- Selina

    昇思MindSpore布道师

### 课程介绍

***【课前学习】 MindSpore Transformers大模型套件：架构讲解与使用入门***

介绍MindSpore Transformers大模型套件现状，讲解套件架构及高阶接口设计，走读工程架构模块代码，学习基本使用方式

[link](https://www.bilibili.com/video/BV1jh4y1m7xV/?spm_id_from=333.999.0.0)


***第一讲：ChatGLM***

介绍技术公开课整体课程安排；ChatGLM模型结构，走读代码演示ChatGLM推理部署


***第二讲：多模态遥感智能解译基础模型***

介绍多模态遥感智能解译基础模型的原理、训推等相关技术，以及模型相关行业应用

***第三讲：ChatGLM2***

介绍ChatGLM2模型结构，走读代码演示ChatGLM推理部署

***第四讲：文本生成解码原理***

介绍Beam search和采样的原理及代码实现

***第五讲：LLAMA***

介绍LLAMA模型结构，走读代码演示推理部署，介绍Alpaca

***第六讲：LLAMA2***

介绍LLAMA2模型结构，走读代码演示LLAMA2 chat部署

***第七讲：云从大模型***

***第八讲：MOE***

***第九讲：CPM***

介绍CPM-Bee预训练、推理、微调及代码现场演示

***第十讲：高效参数微调***

介绍Lora、（P-Tuning）原理及代码实现

***第十一讲：参数微调平台***

***第十二讲：Prompt Engineering***

***第十三讲：量化***

介绍低比特量化等相关模型量化技术

***第十四讲：框架LangChain模块解析***

解析Models、Prompts、Memory、Chains、Agents、Indexes、Callbacks模块，及案例分析

***第十五讲：LangChain对话机器人综合案例***

MindSpore Transformers本地模型与LangChain框架组合使用，通过LangChain框架管理向量库并基于向量库对MindSpore Transformers本地模型问答进行优化

## 大模型专题第一期（已结课）

手把手教你搭建一个简易版ChatGPT

课程资料归档：[link](./Season1.step_into_chatgpt/)

### 教研团队

- 刘群

    华为语音语义首席科学家

- 苏腾 

    昇思MindSpore技术专家 昇思MindSpore超大规模AI架构师

- 夏箫 

    清华大学知识工程实验室博士生 代码生成模型CodeGeeX主要作者之一

- 王金桥 

    中国科学院自动化研究所紫东太初大模型研究中心常务副主任 武汉人工智能研究院院长

- 唐帅 

    武汉人工智能研究院语音算法工程师

- CQU弟中弟 

    昇思MindSpore易用性专家 昇思MindSpore布道师

- Selina 

    昇思MindSpore布道师

### 课程介绍

***第一讲：Transformer***

Multi-head self-attention原理。Masked self-attention的掩码处理方式。基于Transformer的机器翻译任务训练。

***第二讲：BERT***

基于Transformer Encoder的BERT模型设计：MLM和NSP任务。BERT进行下游任务微调的范式。

***第三讲：GPT***

基于Transformer Decoder的GPT模型设计：Next token prediction。GPT下游任务微调范式。

***第四讲：GPT2***

GPT2的核心创新点，包括Task Conditioning和Zero shot learning；模型实现细节基于GPT1的改动。

***第五讲：MindSpore自动并行***

以MindSpore分布式并行特性为依托的数据并行、模型并行、Pipeline并行、内存优化等技术。

***第六讲：代码预训练***

代码预训练发展沿革。Code数据的预处理。CodeGeex代码预训练大模型。

***第七讲：Prompt Tuning***

Pretrain-finetune范式到Prompt tuning范式的改变。Hard prompt和Soft prompt相关技术。只需要改造描述文本的prompting。

***第八讲：多模态预训练大模型***

紫东太初多模态大模型的设计、数据处理和优势；语音识别的理论概述、系统框架和现状及挑战。

***第九讲：Instruction Tuning***

Instruction tuning的核心思想：让模型能够理解任务描述（指令）。Instruction tuning的局限性：无法支持开放域创新性任务、无法对齐LM训练目标和人类需求。Chain-of-thoughts：通过在prompt中提供示例，让模型“举一反三”。

***第十讲：RLHF***

RLHF核心思想：将LLM和人类行为对齐。RLHF技术分解：LLM微调、基于人类反馈训练奖励模型、通过强化学习PPO算法实现模型微调。

